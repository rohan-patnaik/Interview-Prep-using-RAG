{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# RAG Interview Preparation - End-to-End Demo\n",
    "\n",
    "This notebook demonstrates the end-to-end pipeline:\n",
    "\n",
    "1. Audio Recording\n",
    "2. Transcription (Local Whisper or OpenAI API)\n",
    "3. Document Upload & Embedding\n",
    "4. Retrieval from Pinecone\n",
    "5. Feedback Generation (GPT or LLaMA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording for 3 seconds...\n",
      "Recording complete!\n",
      "Transcribing locally with Whisper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohan\\miniconda3\\envs\\torch\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate's transcribed text: \n",
      "Split into 1 chunk(s).\n",
      "Processing 1 chunks for upsert...\n",
      "Processed chunk 1/1\n",
      "Upserted batch 1\n",
      "Documents have been embedded and upserted successfully!\n",
      "L\u0012W[vppPd best match: \u000e0dE\"w!:jXoȊC_K5\fSS\u0016:Y\u001bX+\tj\u0011myV\\o\n",
      "j\"\u0005\u000fB5'M\u001d$\u001dcG\u001b(\u0002<f\u00064'K`̩p\u0015fiJR\u0005\u0006GǅUS\u0002p7Qq.\u0017p~\n",
      "\u001fwѪ\n",
      "K\u0003\n",
      "S)\u0019C\u0004\u0005Hq\u000bėhߺTy&2$ӞΔ'j.Jɷ\u001c\u0006U&\u0016W5JX\u0017N\u001cA\u0004naǭ8WU\u0001C\u00174:ʘ7{1wCmDOU\u0002%9Կ\u0002)l\u0010}MJ\u0003\u001e\\K<{\u0014;w@\u0004_X\f׫m`IZ*\u0003q1\t-˝\u0003ݡ=ۓOrZ􌳿Ś5\u000eY\\:,9[nJsp>v\u000e/5W\tD?SMvhRV~Ѕd\u0019l2\u0014ߚ\\d\"F3\u001ab\u001d\"b\n",
      "渺>嵬\u0001ǁy䨾gu>bWRxt\u00045|kØ\u0014ƶ[.8RO\\VJXj\u0002\u001d(8fr\u0015kRtF'c^Jt)z\u0007=YKz_\u0001ՓI9z{6:.؋v<WvNƾ5~\u001d$lca\u0017f9;2̡RUt:\u0012\u001d\u000e\u00183/[Ɂ.~@\u00182\u0004\u0007|#p?\u000eY]CE\n",
      "zN\u001aw.Yt\u001b,·Bgڐ:s\u0002^w\u0015xC\n",
      "\u0013Q-u\u0003B7j\u0015y\u0013\u0019\u0012\u0015[\u001a\u0006f5.le1o0\u0010\u000f\u000exRA\u000f0\u0016J~cX]\"o_Fm5}$33Maw\u001f}7\u001a\u001a˝8īJ{c,ʎ-\u0006_09Fi&hLp\u0002ܦ^ltlh#.)\u000b\u001bF\u0012$\u001c5,r\u0011,\u001fiF\u0017\u0017j+\u001bpl?co\u000bF0e~\u0000=\u0016E0eL͜KeU|d\u0015\u0010h|rnJaӆ+h\u001aU\u0000oD\u0003f<||JĻ9\u000fY\t@ch5\u0018\u00165K!$|\u0004\u000b7EhlJF}\u0013=ߌbC-\n",
      "ѫ+7*d>3)*)F5|~\u001a2,D8l\\]]\u0003E<TM\"љRVm_G%[\tYDfB7z\u0004=~\u0007Hn$2\u0013޹.RY;^\u0011&CҲ[_\u001dcOi\u000bVu\n",
      "Feedback:\n",
      " | Key Point Missed | Details/Explanation |\n",
      "|------------------|---------------------|\n",
      "| Coherence and relevance | The candidate's answer is completely unrelated and incoherent compared to the ideal answer. They did not provide any relevant information or address the question at all. |\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "import time\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# ... plus Pinecone, etc.\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "\n",
    "GLOBAL_AUDIO_DATA = []\n",
    "GLOBAL_STREAM = None\n",
    "is_recording = False\n",
    "\n",
    "def audio_callback(indata, frames, time_, status):\n",
    "    global GLOBAL_AUDIO_DATA\n",
    "    GLOBAL_AUDIO_DATA.append(indata.copy())\n",
    "\n",
    "def toggle_recording(filename=\"candidate_answer.wav\", fs=44100):\n",
    "    global is_recording, GLOBAL_AUDIO_DATA, GLOBAL_STREAM\n",
    "    if not is_recording:\n",
    "        # Start\n",
    "        print(\"Recording started...\")\n",
    "        GLOBAL_AUDIO_DATA = []\n",
    "        GLOBAL_STREAM = sd.InputStream(samplerate=fs, channels=1, callback=audio_callback)\n",
    "        GLOBAL_STREAM.start()\n",
    "        is_recording = True\n",
    "    else:\n",
    "        # Stop\n",
    "        GLOBAL_STREAM.stop()\n",
    "        is_recording = False\n",
    "        audio_data = np.concatenate(GLOBAL_AUDIO_DATA, axis=0)\n",
    "        write(filename, fs, audio_data)\n",
    "        print(f\"Recording stopped. Saved to {filename}\")\n",
    "        return filename\n",
    "\n",
    "# Example usage in the notebook:\n",
    "print(\"Press Enter to start recording...\")\n",
    "input()\n",
    "toggle_recording()  # starts recording\n",
    "\n",
    "print(\"Press Enter to stop recording...\")\n",
    "input()\n",
    "audio_file = toggle_recording()  # stops recording and returns filename\n",
    "\n",
    "# Then transcribe:\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(audio_file)\n",
    "candidate_text = result[\"text\"]\n",
    "print(\"Transcribed text:\", candidate_text)\n",
    "\n",
    "# The rest is your normal logic for chunking, Pinecone upsert, retrieval, etc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
